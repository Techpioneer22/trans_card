# -*- coding: utf-8 -*-
"""Project Banking Trans_Fraud fraud.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u5mUcEggsibjgdwMyb21Yd1HrpjEprnW
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.utils import shuffle
from sklearn.preprocessing import MinMaxScaler 
from keras.layers import Dense
from keras.layers import Dropout
import seaborn as sn
import matplotlib as pt

"""Compiled by:
1.  Perceviarance Chitima R195807D
2. Emmanuel Banda R197226Y
3. Agness Mashonganyika R195845Q

                Credit card fraud Detection
                
"""

from keras.models import Sequential
from google.colab import drive
drive.mount('/content/drive')

df_full =pd.read_csv('/content/drive/My Drive/Colab Notebooks/creditcard.csv')

df_full.describe()

df_full.head(5)

df_full.Class.value_counts()

df_full.sort_values( by='Class', ascending= False , inplace=True)

df_full.drop('Time', axis=1, inplace=True)

#Assigning the first 3000 samples to a new dataframe
df_sample = df_full.iloc[:3000, :]

df_sample.Class.value_counts()

shuffle_df = shuffle(df_sample, random_state= 42)

#slitting data into 2 .. training and testing segments
df_train = shuffle_df[0:2400]
df_test = shuffle_df[2400: ]

#Spliting each dataframe into feature and label
train_feature = np.array(df_train.values[:, 0:29])
train_label = np.array(df_train.values[:, -1])
test_feature = np.array(df_train.values[:, 0:29])
test_label = np.array(df_train.values[:, -1])

train_feature.shape

train_label.shape

scaler = MinMaxScaler()
scaler.fit(train_feature)
train_feature_trans=scaler.transform(train_feature)
train_feature_trans = scaler.transform(test_feature)

model= Sequential()

model.add(Dense(units=200,input_dim=29,kernel_initializer='uniform', activation='relu'))

model.add(Dropout(0.5))

model.add(Dense(units=200,kernel_initializer='uniform', activation='relu'))

model.add(Dropout(0.5))

model.add(Dense(units=1,kernel_initializer='uniform', activation='sigmoid'))

model.summary()

model.compile(loss='binary_crossentropy', optimizer='adam',
metrics=['accuracy'])

train_history = model.fit(x=train_feature_trans, y=train_label,
validation_split=0.8, epochs=200,
batch_size=500, verbose=2)



#!pip install streamlit

#!pip install pyngrok

#%%writefile app.py
import streamlit as st

#!ls

#!ngrok authtoken xxxxx

#!ngrok

#from pyngrok import ngrok
#!streamlit run app.py
#public_url = ngrok.connect(port = '8080')#

#!pgrep streamlit

#public_url

#!ngrok kill